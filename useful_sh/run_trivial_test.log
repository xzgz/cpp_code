INFO 07-15 03:55:17 [__init__.py:248] Automatically detected platform rocm.
<module 'vllm' from '/data/heyanguang/code/vllm_fa_batch_prefill/rocm_vllm/vllm/__init__.py'>
0.6.1.dev3865+g8ef30c4d0.d20250712
WARNING 07-15 03:55:25 [config.py:3064] Casting torch.bfloat16 to torch.float16.
INFO 07-15 03:55:42 [config.py:760] This model supports multiple tasks: {'generate', 'reward', 'classify', 'embed', 'score'}. Defaulting to 'generate'.
WARNING 07-15 03:55:42 [arg_utils.py:1546] Detected VLLM_USE_V1=1 with rocm. Usage should be considered experimental. Please report any issues on Github.
INFO 07-15 03:55:42 [config.py:2072] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 07-15 03:55:42 [fp8.py:66] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.
WARNING 07-15 03:55:42 [utils.py:2455] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized
INFO 07-15 03:55:46 [__init__.py:248] Automatically detected platform rocm.
INFO 07-15 03:55:54 [core.py:61] Initializing a V1 LLM engine (v0.6.1.dev3865+g8ef30c4d0.d20250712) with config: model='/data/heyanguang/code/models/Llama-3.1-8B-Instruct-FP8-KV', speculative_config=None, tokenizer='/data/heyanguang/code/models/Llama-3.1-8B-Instruct-FP8-KV', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/heyanguang/code/models/Llama-3.1-8B-Instruct-FP8-KV, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level": 3, "custom_ops": ["none"], "splitting_ops": ["vllm.unified_attention", "vllm.unified_attention_with_output"], "compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "use_cudagraph": true, "cudagraph_num_of_warmups": 1, "cudagraph_capture_sizes": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 512}
WARNING 07-15 03:55:54 [utils.py:2595] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ed8dca51f70>
INFO 07-15 03:55:54 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 07-15 03:55:54 [rocm.py:195] Using Flash Attention backend on V1 engine.
[aiter] WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
[2025-07-15 03:55:54] WARNING core.py:198: WARNING: NUMA balancing is enabled, which may cause errors. It is recommended to disable NUMA balancing by running "sudo sh -c 'echo 0 > /proc/sys/kernel/numa_balancing'" for more details: https://rocm.docs.amd.com/en/latest/how-to/system-optimization/mi300x.html#disable-numa-auto-balancing
INFO 07-15 03:55:55 [gpu_model_runner.py:1385] Starting to load model /data/heyanguang/code/models/Llama-3.1-8B-Instruct-FP8-KV...
INFO 07-15 03:55:55 [rocm.py:195] Using Flash Attention backend on V1 engine.
INFO 07-15 03:55:55 [backends.py:36] Using InductorAdaptor
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
WARNING 07-15 03:55:55 [weight_utils.py:704] DEPRECATED. Found kv_scale in the checkpoint. This format is deprecated in favor of separate k_scale and v_scale tensors and will be removed in a future release. Functionally, we will remap kv_scale to k_scale and duplicate k_scale to v_scale
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.62s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.57s/it]

INFO 07-15 03:55:58 [default_loader.py:278] Loading weights took 3.41 seconds
INFO 07-15 03:55:59 [gpu_model_runner.py:1403] Model loading took 10.1284 GiB and 4.090330 seconds
INFO 07-15 03:56:07 [backends.py:458] Using cache directory: /root/.cache/vllm/torch_compile_cache/f8791b1887/rank_0_0 for vLLM's torch.compile
INFO 07-15 03:56:07 [backends.py:468] Dynamo bytecode transform time: 7.98 s
INFO 07-15 03:56:29 [backends.py:133] Directly load the compiled graph(s) for shape None from the cache, took 21.183 s
INFO 07-15 03:56:30 [monitor.py:33] torch.compile takes 7.98 s in total
INFO 07-15 03:56:32 [kv_cache_utils.py:639] GPU KV cache size: 1,283,600 tokens
INFO 07-15 03:56:32 [kv_cache_utils.py:642] Maximum concurrency for 131,072 tokens per request: 9.79x
INFO 07-15 03:56:53 [gpu_model_runner.py:1766] Graph capturing finished in 21 secs, took 2.65 GiB
INFO 07-15 03:56:53 [core.py:163] init engine (profile, create kv cache, warmup model) took 53.76 seconds
INFO 07-15 03:56:53 [core_client.py:442] Core engine process 0 ready.
Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]Adding requests: 100%|██████████| 4/4 [00:00<00:00, 1290.16it/s]
Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][aiter] type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, is_chunked_prefill: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[2025-07-15 03:56:53] WARNING core.py:583: type hints mismatch, override to --> mha_batch_prefill(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, cu_seqlens_q: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, max_seqlen_q: int, max_seqlen_k: int, dropout_p: float, softmax_scale: float, logits_soft_cap: float, zero_tensors: bool, is_causal: bool, window_size_left: int, window_size_right: int, return_softmax_lse: bool, return_dropout_randval: bool, is_chunked_prefill: bool, out: Optional[torch.Tensor] = None, bias: Optional[torch.Tensor] = None, alibi_slopes: Optional[torch.Tensor] = None, gen: Optional[torch.Generator] = None) -> list[torch.Tensor]
[aiter] type hints mismatch, override to --> paged_attention_v1(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: float, arg6: torch.Tensor, arg7: torch.Tensor, arg8: torch.Tensor, arg9: int, arg10: Optional[torch.Tensor], arg11: str, arg12: str, arg13: float, arg14: torch.Tensor, arg15: torch.Tensor, arg16: Optional[torch.Tensor], arg17: int) -> None
[2025-07-15 03:56:53] WARNING core.py:583: type hints mismatch, override to --> paged_attention_v1(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: float, arg6: torch.Tensor, arg7: torch.Tensor, arg8: torch.Tensor, arg9: int, arg10: Optional[torch.Tensor], arg11: str, arg12: str, arg13: float, arg14: torch.Tensor, arg15: torch.Tensor, arg16: Optional[torch.Tensor], arg17: int) -> None
Processed prompts:  25%|██▌       | 1/4 [00:01<00:05,  1.90s/it, est. speed input: 3.16 toks/s, output: 52.74 toks/s]Processed prompts: 100%|██████████| 4/4 [00:01<00:00,  2.09it/s, est. speed input: 13.61 toks/s, output: 209.39 toks/s]

Generated Outputs:
------------------------------------------------------------
Prompt:    'Hello, my name is'
Output:    ' Emily and I am a senior in the College of Arts and Sciences majoring in psychology. As I am nearing the end of my undergraduate career, I have come to realize how much I have learned and grown throughout my time at Notre Dame. As a member of the Class of 2023, I am excited to reflect on my experiences and share them with the community.\n\nAs a psychology major, I have been fortunate to take a variety of courses that have allowed me to explore different areas of the field'
------------------------------------------------------------
Prompt:    'The president of the United States is'
Output:    " the head of state and head of government of the United States. The president is elected by the people through the Electoral College and serves a four-year term. The president's powers and duties are defined by the U.S. Constitution and other laws.\nThe president's primary responsibilities include:\nConducting the nation's foreign policy\nNegotiating treaties and agreements with other countries\nAppointing federal judges, ambassadors, and other high-ranking officials\nSigning or vetoing bills passed by Congress\nGranting"
------------------------------------------------------------
Prompt:    'The capital of France is'
Output:    ' the most visited city in the world with over 23 million tourists in 2017. Paris is a beautiful city with stunning architecture, world-class museums, and vibrant cultural scene. If you are planning to visit Paris for the first time, here are some tips to help you make the most of your trip:\n1. Learn a few French phrases: While many Parisians speak English, it is always helpful to learn a few basic French phrases, such as "bonjour" (hello), "mer'
------------------------------------------------------------
Prompt:    'The future of AI is'
Output:    ' all about data and collaboration\nThe future of artificial intelligence (AI) is all about data and collaboration, according to a panel discussion at the AI Summit in London last week.\nAI is all about making decisions based on data, and the more data you have, the better you can make those decisions, said Alistair Johnston, director of product and innovation at Cognizant.\nBut AI is not just about data, it’s also about collaboration, he said. The future of AI will be'
------------------------------------------------------------
[rank0]:[W715 03:56:56.682184906 ProcessGroupNCCL.cpp:1487] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
INFO 07-15 04:07:17 [__init__.py:248] Automatically detected platform rocm.
<module 'vllm' from '/data/heyanguang/code/vllm_fa_batch_prefill/rocm_vllm/vllm/__init__.py'>
0.6.1.dev3865+g8ef30c4d0.d20250712
WARNING 07-15 04:07:19 [config.py:3064] Casting torch.bfloat16 to torch.float16.
INFO 07-15 04:07:36 [config.py:760] This model supports multiple tasks: {'embed', 'classify', 'reward', 'score', 'generate'}. Defaulting to 'generate'.
INFO 07-15 04:07:42 [arg_utils.py:1552] rocm is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.
WARNING 07-15 04:07:42 [arg_utils.py:1389] The model has a long context length (131072). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.
WARNING 07-15 04:07:42 [fp8.py:66] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.
INFO 07-15 04:07:42 [llm_engine.py:240] Initializing a V0 LLM engine (v0.6.1.dev3865+g8ef30c4d0.d20250712) with config: model='/data/heyanguang/code/models/Llama-3.1-8B-Instruct-FP8-KV', speculative_config=None, tokenizer='/data/heyanguang/code/models/Llama-3.1-8B-Instruct-FP8-KV', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data/heyanguang/code/models/Llama-3.1-8B-Instruct-FP8-KV, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={"compile_sizes": [], "inductor_compile_config": {"enable_auto_functionalized_v2": false}, "cudagraph_capture_sizes": [256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], "max_capture_size": 256}, use_cached_outputs=False, 
INFO 07-15 04:07:43 [rocm.py:207] None is not supported in AMD GPUs.
INFO 07-15 04:07:43 [rocm.py:208] Using ROCmFlashAttention backend.
INFO 07-15 04:07:43 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 07-15 04:07:43 [model_runner.py:1161] Starting to load model /data/heyanguang/code/models/Llama-3.1-8B-Instruct-FP8-KV...
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
WARNING 07-15 04:07:44 [weight_utils.py:704] DEPRECATED. Found kv_scale in the checkpoint. This format is deprecated in favor of separate k_scale and v_scale tensors and will be removed in a future release. Functionally, we will remap kv_scale to k_scale and duplicate k_scale to v_scale
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.35s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.71s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.65s/it]

INFO 07-15 04:07:47 [default_loader.py:278] Loading weights took 3.56 seconds
INFO 07-15 04:07:48 [model_runner.py:1193] Model loading took 10.2729 GiB and 4.342114 seconds
INFO 07-15 04:08:04 [worker.py:288] Memory profiling takes 16.05 seconds
INFO 07-15 04:08:04 [worker.py:288] the current vLLM instance can use total_gpu_memory (191.98GiB) x gpu_memory_utilization (0.90) = 172.79GiB
INFO 07-15 04:08:04 [worker.py:288] model weights take 10.27GiB; non_torch_memory takes 0.67GiB; PyTorch activation peak memory takes 13.50GiB; the rest of the memory reserved for KV Cache is 148.34GiB.
INFO 07-15 04:08:04 [executor_base.py:112] # rocm blocks: 75948, # CPU blocks: 2048
INFO 07-15 04:08:04 [executor_base.py:117] Maximum concurrency for 131072 tokens per request: 9.27x
INFO 07-15 04:08:05 [model_runner.py:1503] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:16,  2.11it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:12,  2.56it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:11,  2.79it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:11,  2.77it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:01<00:11,  2.66it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:10,  2.80it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:02<00:09,  2.90it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:02<00:09,  2.98it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:03<00:08,  2.98it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:03<00:08,  3.02it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:03<00:07,  3.05it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:04<00:07,  2.94it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:04<00:07,  2.96it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:04<00:07,  3.00it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:05<00:06,  3.01it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:05<00:06,  3.02it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:05<00:06,  2.92it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:06<00:05,  2.95it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:06<00:05,  3.00it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:06<00:04,  3.05it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:07<00:04,  3.08it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:07<00:04,  3.09it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:07<00:03,  3.11it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:08<00:03,  3.10it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:08<00:03,  3.12it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:08<00:02,  3.10it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:09<00:02,  3.09it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:09<00:02,  3.11it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:09<00:01,  3.09it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:10<00:01,  3.00it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:10<00:01,  2.94it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:10<00:01,  2.99it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:11<00:00,  2.98it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:11<00:00,  3.00it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  3.14it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:11<00:00,  2.99it/s]
INFO 07-15 04:08:16 [model_runner.py:1661] Graph capturing finished in 12 secs, took 0.24 GiB
INFO 07-15 04:08:16 [llm_engine.py:435] init engine (profile, create kv cache, warmup model) took 28.92 seconds
Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]Adding requests: 100%|██████████| 4/4 [00:00<00:00, 1858.56it/s]
Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  25%|██▌       | 1/4 [00:03<00:11,  3.92s/it, est. speed input: 1.53 toks/s, output: 25.51 toks/s]Processed prompts: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s, est. speed input: 6.63 toks/s, output: 102.03 toks/s]
[rank0]:[W715 04:08:23.364678416 collection.cpp:1100] Warning: ROCTracer produced duplicate flow start: 2 (function operator())

Generated Outputs:
------------------------------------------------------------
Prompt:    'Hello, my name is'
Output:    ' Princess Luna and I am 13 years old. I live in Ghana, a beautiful country in West Africa. I am a student at a Catholic school and I attend the Divine Mercy Academy.\nI love reading, and my favorite book is "The Great Gatsby" by F. Scott Fitzgerald. I love the way the author used descriptive language and the way the characters\' personalities are portrayed. I also love reading about historical events and how they shaped the world we live in today.\nIn my free time'
------------------------------------------------------------
Prompt:    'The president of the United States is'
Output:    ' not just the leader of the country, but also the head of state and head of government. This is a unique position, and the person who holds this role has a great deal of power and influence. But what does it take to be a good president? Here are some key qualities and characteristics that can help someone succeed in this role:\n\n1.  **Leadership**: A good president must be able to inspire and motivate others to work towards a common goal. This involves setting a positive example,'
------------------------------------------------------------
Prompt:    'The capital of France is'
Output:    " a beautiful city, full of rich history, art, fashion, cuisine and romance. Paris is a must-visit destination for any traveler. It is a city that will leave you spellbound and longing for more. Let's explore the wonders of the City of Light, with a guided tour.\nDay 1: Arrival in Paris\nDay 1 of our guided tour in Paris starts with your arrival at Charles de Gaulle airport. We will pick you up and transfer you to your hotel. After"
------------------------------------------------------------
Prompt:    'The future of AI is'
Output:    ' bright, and it’s only a matter of time before AI surpasses human intelligence. However, there is a catch. While AI has made tremendous progress, it is still far from achieving human-level intelligence. There are several reasons why AI has not yet reached the level of human intelligence, and some of these reasons include the complexity of the human brain, the lack of a clear understanding of human intelligence, and the limitations of current AI systems.\nAI systems are programmed to perform specific tasks and are not capable'
------------------------------------------------------------
[rank0]:[W715 04:08:28.295542016 ProcessGroupNCCL.cpp:1487] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
